{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdaa22e",
   "metadata": {},
   "source": [
    "# Local AI Assistant Project â€“ CLI Commands\n",
    "\n",
    "This document contains all command-line operations used to deploy a local AI assistant.\n",
    "The commands work on **Windows (PowerShell)** and **macOS (Terminal)** unless stated otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Verify Ollama Installation\n",
    "\n",
    "### Windows (PowerShell) / macOS (Terminal)\n",
    "```bash\n",
    "ollama --version\n",
    "````\n",
    "\n",
    "Expected output: Ollama version information.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Download the LLaMA 3.2 Model (1.24B)\n",
    "\n",
    "This command downloads a lightweight large language model suitable for local execution.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:1.24b\n",
    "```\n",
    "\n",
    "Wait until the download completes successfully.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Run the LLaMA Model Locally\n",
    "\n",
    "This command starts the AI assistant in the terminal.\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2:1.24b\n",
    "```\n",
    "\n",
    "You should now be able to type prompts and receive responses directly in the CLI.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Verify Docker Installation\n",
    "\n",
    "### Windows (PowerShell) / macOS (Terminal)\n",
    "\n",
    "```bash\n",
    "docker --version\n",
    "```\n",
    "\n",
    "Expected output: Docker version information.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Run Open WebUI Using Docker\n",
    "\n",
    "Open WebUI provides a web-based interface to interact with the local AI assistant.\n",
    "\n",
    "### Windows (PowerShell)\n",
    "\n",
    "```powershell\n",
    "docker run -d `\n",
    "  -p 3000:8080 `\n",
    "  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 `\n",
    "  --name open-webui `\n",
    "  ghcr.io/open-webui/open-webui:main\n",
    "```\n",
    "\n",
    "### macOS (Terminal)\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  -p 3000:8080 \\\n",
    "  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \\\n",
    "  --name open-webui \\\n",
    "  ghcr.io/open-webui/open-webui:main\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Access the Web Interface\n",
    "\n",
    "Open a web browser and navigate to:\n",
    "\n",
    "```\n",
    "http://localhost:3000\n",
    "```\n",
    "\n",
    "You should see the Open WebUI interface connected to Ollama.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Verify Running Containers (Optional)\n",
    "\n",
    "```bash\n",
    "docker ps\n",
    "```\n",
    "\n",
    "This command lists running Docker containers and confirms that Open WebUI is active.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Stop and Remove Open WebUI (Optional Cleanup)\n",
    "\n",
    "### Stop the container\n",
    "\n",
    "```bash\n",
    "docker stop open-webui\n",
    "```\n",
    "\n",
    "### Remove the container\n",
    "\n",
    "```bash\n",
    "docker rm open-webui\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "* No programming code was written for this project.\n",
    "* All functionality is achieved through configuration and command-line operations.\n",
    "* Screenshots should be taken after each major step for documentation purposes.\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
